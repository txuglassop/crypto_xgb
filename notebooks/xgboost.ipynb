{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build upon findings of the `xgboost` model in `classification_models.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path(\"..\", \"src\")\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "from feature_engineering import add_vwap, add_atr, add_ema, add_dow, add_return, add_jump_categories_3, add_jump_categories_5\n",
    "from utility_functions import classification_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do all the data cleaning / feature engineering and `xgboost` specific preprocessing as seen in the aforementioned notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43817 entries, 0 to 43816\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   date                      43817 non-null  float64\n",
      " 1   open                      43817 non-null  float64\n",
      " 2   high                      43817 non-null  float64\n",
      " 3   low                       43817 non-null  float64\n",
      " 4   close                     43817 non-null  float64\n",
      " 5   volume                    43817 non-null  float64\n",
      " 6   base_asset_volume         43817 non-null  float64\n",
      " 7   no_trades                 43817 non-null  int64  \n",
      " 8   taker_buy_vol             43817 non-null  float64\n",
      " 9   taker_buy_base_asset_vol  43817 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.3 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>base_asset_volume</th>\n",
       "      <th>no_trades</th>\n",
       "      <th>taker_buy_vol</th>\n",
       "      <th>taker_buy_base_asset_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>129.16</td>\n",
       "      <td>129.19</td>\n",
       "      <td>128.68</td>\n",
       "      <td>128.87</td>\n",
       "      <td>7769.17336</td>\n",
       "      <td>1.000930e+06</td>\n",
       "      <td>2504</td>\n",
       "      <td>4149.93345</td>\n",
       "      <td>534619.3390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>128.87</td>\n",
       "      <td>130.65</td>\n",
       "      <td>128.78</td>\n",
       "      <td>130.64</td>\n",
       "      <td>11344.65516</td>\n",
       "      <td>1.474278e+06</td>\n",
       "      <td>4885</td>\n",
       "      <td>5930.54276</td>\n",
       "      <td>770486.0567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>130.63</td>\n",
       "      <td>130.98</td>\n",
       "      <td>130.35</td>\n",
       "      <td>130.85</td>\n",
       "      <td>7603.35623</td>\n",
       "      <td>9.940256e+05</td>\n",
       "      <td>3046</td>\n",
       "      <td>3324.35218</td>\n",
       "      <td>434675.4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.577850e+12</td>\n",
       "      <td>130.85</td>\n",
       "      <td>130.89</td>\n",
       "      <td>129.94</td>\n",
       "      <td>130.20</td>\n",
       "      <td>4968.55433</td>\n",
       "      <td>6.473610e+05</td>\n",
       "      <td>2818</td>\n",
       "      <td>1810.03564</td>\n",
       "      <td>235890.3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.577850e+12</td>\n",
       "      <td>130.21</td>\n",
       "      <td>130.74</td>\n",
       "      <td>130.15</td>\n",
       "      <td>130.20</td>\n",
       "      <td>3397.90747</td>\n",
       "      <td>4.430067e+05</td>\n",
       "      <td>2264</td>\n",
       "      <td>1839.74371</td>\n",
       "      <td>239848.3483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    open    high     low   close       volume  \\\n",
       "0  1.577840e+12  129.16  129.19  128.68  128.87   7769.17336   \n",
       "1  1.577840e+12  128.87  130.65  128.78  130.64  11344.65516   \n",
       "2  1.577840e+12  130.63  130.98  130.35  130.85   7603.35623   \n",
       "3  1.577850e+12  130.85  130.89  129.94  130.20   4968.55433   \n",
       "4  1.577850e+12  130.21  130.74  130.15  130.20   3397.90747   \n",
       "\n",
       "   base_asset_volume  no_trades  taker_buy_vol  taker_buy_base_asset_vol  \n",
       "0       1.000930e+06       2504     4149.93345               534619.3390  \n",
       "1       1.474278e+06       4885     5930.54276               770486.0567  \n",
       "2       9.940256e+05       3046     3324.35218               434675.4447  \n",
       "3       6.473610e+05       2818     1810.03564               235890.3302  \n",
       "4       4.430067e+05       2264     1839.74371               239848.3483  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"./../input/ETHUSDT_1h_2020_2024_join_final.csv\")\n",
    "\n",
    "df_raw.drop(df_raw.columns[df_raw.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "print(df_raw.info())\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date float64\n",
      "open float64\n",
      "high float64\n",
      "low float64\n",
      "close float64\n",
      "volume float64\n",
      "base_asset_volume float64\n",
      "no_trades int64\n",
      "taker_buy_vol float64\n",
      "taker_buy_base_asset_vol float64\n",
      "return float64\n",
      "atr float64\n",
      "ema float64\n",
      "VWAP float64\n",
      "dow_Monday bool\n",
      "dow_Saturday bool\n",
      "dow_Sunday bool\n",
      "dow_Thursday bool\n",
      "dow_Tuesday bool\n",
      "dow_Wednesday bool\n",
      "open_1 float64\n",
      "high_1 float64\n",
      "low_1 float64\n",
      "close_1 float64\n",
      "volume_1 float64\n",
      "atr_1 float64\n",
      "ema_1 float64\n",
      "VWAP_1 float64\n",
      "open_2 float64\n",
      "high_2 float64\n",
      "low_2 float64\n",
      "close_2 float64\n",
      "volume_2 float64\n",
      "atr_2 float64\n",
      "ema_2 float64\n",
      "VWAP_2 float64\n",
      "open_3 float64\n",
      "high_3 float64\n",
      "low_3 float64\n",
      "close_3 float64\n",
      "volume_3 float64\n",
      "atr_3 float64\n",
      "ema_3 float64\n",
      "VWAP_3 float64\n",
      "open_4 float64\n",
      "high_4 float64\n",
      "low_4 float64\n",
      "close_4 float64\n",
      "volume_4 float64\n",
      "atr_4 float64\n",
      "ema_4 float64\n",
      "VWAP_4 float64\n",
      "open_5 float64\n",
      "high_5 float64\n",
      "low_5 float64\n",
      "close_5 float64\n",
      "volume_5 float64\n",
      "atr_5 float64\n",
      "ema_5 float64\n",
      "VWAP_5 float64\n",
      "jump_neutral bool\n",
      "jump_up bool\n",
      "next_jump object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>base_asset_volume</th>\n",
       "      <th>no_trades</th>\n",
       "      <th>taker_buy_vol</th>\n",
       "      <th>taker_buy_base_asset_vol</th>\n",
       "      <th>...</th>\n",
       "      <th>high_5</th>\n",
       "      <th>low_5</th>\n",
       "      <th>close_5</th>\n",
       "      <th>volume_5</th>\n",
       "      <th>atr_5</th>\n",
       "      <th>ema_5</th>\n",
       "      <th>VWAP_5</th>\n",
       "      <th>jump_neutral</th>\n",
       "      <th>jump_up</th>\n",
       "      <th>next_jump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.577910e+12</td>\n",
       "      <td>132.04</td>\n",
       "      <td>132.16</td>\n",
       "      <td>131.62</td>\n",
       "      <td>131.86</td>\n",
       "      <td>2111.21443</td>\n",
       "      <td>2.783557e+05</td>\n",
       "      <td>1995</td>\n",
       "      <td>997.52946</td>\n",
       "      <td>1.315025e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.24</td>\n",
       "      <td>131.96</td>\n",
       "      <td>7325.25762</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>131.029332</td>\n",
       "      <td>130.561825</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.577910e+12</td>\n",
       "      <td>131.86</td>\n",
       "      <td>132.25</td>\n",
       "      <td>131.70</td>\n",
       "      <td>132.18</td>\n",
       "      <td>2014.79285</td>\n",
       "      <td>2.660484e+05</td>\n",
       "      <td>1988</td>\n",
       "      <td>1021.42474</td>\n",
       "      <td>1.349028e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.40</td>\n",
       "      <td>131.60</td>\n",
       "      <td>132.08</td>\n",
       "      <td>5361.06926</td>\n",
       "      <td>0.113214</td>\n",
       "      <td>131.228799</td>\n",
       "      <td>130.641168</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.577910e+12</td>\n",
       "      <td>132.17</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.68</td>\n",
       "      <td>131.78</td>\n",
       "      <td>4879.42025</td>\n",
       "      <td>6.440060e+05</td>\n",
       "      <td>2410</td>\n",
       "      <td>1841.37772</td>\n",
       "      <td>2.429792e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.95</td>\n",
       "      <td>131.78</td>\n",
       "      <td>132.85</td>\n",
       "      <td>6915.20906</td>\n",
       "      <td>0.091658</td>\n",
       "      <td>131.488372</td>\n",
       "      <td>130.764300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.577920e+12</td>\n",
       "      <td>131.82</td>\n",
       "      <td>131.82</td>\n",
       "      <td>129.90</td>\n",
       "      <td>130.27</td>\n",
       "      <td>14876.06749</td>\n",
       "      <td>1.943372e+06</td>\n",
       "      <td>6386</td>\n",
       "      <td>5520.77235</td>\n",
       "      <td>7.209741e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>133.05</td>\n",
       "      <td>132.27</td>\n",
       "      <td>132.34</td>\n",
       "      <td>5424.00732</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>131.701365</td>\n",
       "      <td>130.851473</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.577920e+12</td>\n",
       "      <td>130.28</td>\n",
       "      <td>130.87</td>\n",
       "      <td>129.74</td>\n",
       "      <td>130.77</td>\n",
       "      <td>3865.45991</td>\n",
       "      <td>5.035546e+05</td>\n",
       "      <td>3232</td>\n",
       "      <td>2025.11315</td>\n",
       "      <td>2.639359e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.46</td>\n",
       "      <td>131.57</td>\n",
       "      <td>132.04</td>\n",
       "      <td>5707.79340</td>\n",
       "      <td>0.068019</td>\n",
       "      <td>131.765758</td>\n",
       "      <td>130.908630</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.577920e+12</td>\n",
       "      <td>130.72</td>\n",
       "      <td>130.78</td>\n",
       "      <td>130.27</td>\n",
       "      <td>130.67</td>\n",
       "      <td>3772.66670</td>\n",
       "      <td>4.925267e+05</td>\n",
       "      <td>2565</td>\n",
       "      <td>2094.53022</td>\n",
       "      <td>2.734273e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.16</td>\n",
       "      <td>131.62</td>\n",
       "      <td>131.86</td>\n",
       "      <td>2111.21443</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>131.788607</td>\n",
       "      <td>130.925844</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.577930e+12</td>\n",
       "      <td>130.66</td>\n",
       "      <td>130.67</td>\n",
       "      <td>130.12</td>\n",
       "      <td>130.15</td>\n",
       "      <td>3684.51912</td>\n",
       "      <td>4.803441e+05</td>\n",
       "      <td>2414</td>\n",
       "      <td>1879.43297</td>\n",
       "      <td>2.450120e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.25</td>\n",
       "      <td>131.70</td>\n",
       "      <td>132.18</td>\n",
       "      <td>2014.79285</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>131.839552</td>\n",
       "      <td>130.944429</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.577930e+12</td>\n",
       "      <td>130.14</td>\n",
       "      <td>130.16</td>\n",
       "      <td>128.89</td>\n",
       "      <td>129.72</td>\n",
       "      <td>19078.42209</td>\n",
       "      <td>2.469243e+06</td>\n",
       "      <td>8599</td>\n",
       "      <td>10251.27762</td>\n",
       "      <td>1.326941e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.68</td>\n",
       "      <td>131.78</td>\n",
       "      <td>4879.42025</td>\n",
       "      <td>0.052313</td>\n",
       "      <td>131.860308</td>\n",
       "      <td>130.983103</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.577930e+12</td>\n",
       "      <td>129.71</td>\n",
       "      <td>129.71</td>\n",
       "      <td>128.77</td>\n",
       "      <td>129.10</td>\n",
       "      <td>11950.18634</td>\n",
       "      <td>1.544526e+06</td>\n",
       "      <td>5294</td>\n",
       "      <td>6776.08848</td>\n",
       "      <td>8.759077e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>131.82</td>\n",
       "      <td>129.90</td>\n",
       "      <td>130.27</td>\n",
       "      <td>14876.06749</td>\n",
       "      <td>0.140880</td>\n",
       "      <td>131.620913</td>\n",
       "      <td>130.949343</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.577940e+12</td>\n",
       "      <td>129.09</td>\n",
       "      <td>129.87</td>\n",
       "      <td>128.69</td>\n",
       "      <td>129.55</td>\n",
       "      <td>8931.67759</td>\n",
       "      <td>1.156161e+06</td>\n",
       "      <td>4813</td>\n",
       "      <td>4789.83655</td>\n",
       "      <td>6.200412e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>130.87</td>\n",
       "      <td>129.74</td>\n",
       "      <td>130.77</td>\n",
       "      <td>3865.45991</td>\n",
       "      <td>0.090777</td>\n",
       "      <td>131.388731</td>\n",
       "      <td>130.936277</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    open    high     low   close       volume  \\\n",
       "19  1.577910e+12  132.04  132.16  131.62  131.86   2111.21443   \n",
       "20  1.577910e+12  131.86  132.25  131.70  132.18   2014.79285   \n",
       "21  1.577910e+12  132.17  132.37  131.68  131.78   4879.42025   \n",
       "22  1.577920e+12  131.82  131.82  129.90  130.27  14876.06749   \n",
       "23  1.577920e+12  130.28  130.87  129.74  130.77   3865.45991   \n",
       "24  1.577920e+12  130.72  130.78  130.27  130.67   3772.66670   \n",
       "25  1.577930e+12  130.66  130.67  130.12  130.15   3684.51912   \n",
       "26  1.577930e+12  130.14  130.16  128.89  129.72  19078.42209   \n",
       "27  1.577930e+12  129.71  129.71  128.77  129.10  11950.18634   \n",
       "28  1.577940e+12  129.09  129.87  128.69  129.55   8931.67759   \n",
       "\n",
       "    base_asset_volume  no_trades  taker_buy_vol  taker_buy_base_asset_vol  \\\n",
       "19       2.783557e+05       1995      997.52946              1.315025e+05   \n",
       "20       2.660484e+05       1988     1021.42474              1.349028e+05   \n",
       "21       6.440060e+05       2410     1841.37772              2.429792e+05   \n",
       "22       1.943372e+06       6386     5520.77235              7.209741e+05   \n",
       "23       5.035546e+05       3232     2025.11315              2.639359e+05   \n",
       "24       4.925267e+05       2565     2094.53022              2.734273e+05   \n",
       "25       4.803441e+05       2414     1879.43297              2.450120e+05   \n",
       "26       2.469243e+06       8599    10251.27762              1.326941e+06   \n",
       "27       1.544526e+06       5294     6776.08848              8.759077e+05   \n",
       "28       1.156161e+06       4813     4789.83655              6.200412e+05   \n",
       "\n",
       "    ...  high_5   low_5  close_5     volume_5     atr_5       ema_5  \\\n",
       "19  ...  132.37  131.24   131.96   7325.25762  0.785000  131.029332   \n",
       "20  ...  132.40  131.60   132.08   5361.06926  0.113214  131.228799   \n",
       "21  ...  132.95  131.78   132.85   6915.20906  0.091658  131.488372   \n",
       "22  ...  133.05  132.27   132.34   5424.00732  0.062261  131.701365   \n",
       "23  ...  132.46  131.57   132.04   5707.79340  0.068019  131.765758   \n",
       "24  ...  132.16  131.62   131.86   2111.21443  0.043430  131.788607   \n",
       "25  ...  132.25  131.70   132.18   2014.79285  0.042388  131.839552   \n",
       "26  ...  132.37  131.68   131.78   4879.42025  0.052313  131.860308   \n",
       "27  ...  131.82  129.90   130.27  14876.06749  0.140880  131.620913   \n",
       "28  ...  130.87  129.74   130.77   3865.45991  0.090777  131.388731   \n",
       "\n",
       "        VWAP_5  jump_neutral  jump_up  next_jump  \n",
       "19  130.561825          True    False    neutral  \n",
       "20  130.641168          True    False    neutral  \n",
       "21  130.764300          True    False       down  \n",
       "22  130.851473         False    False    neutral  \n",
       "23  130.908630          True    False    neutral  \n",
       "24  130.925844          True    False    neutral  \n",
       "25  130.944429          True    False       down  \n",
       "26  130.983103         False    False       down  \n",
       "27  130.949343         False    False    neutral  \n",
       "28  130.936277          True    False    neutral  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general data cleaning\n",
    "df = df_raw.copy()\n",
    "\n",
    "df = add_return(df)\n",
    "\n",
    "# add jump feature and target variable\n",
    "df = add_jump_categories_3(df, up_margin=0.005, down_margin=0.003)\n",
    "df['next_jump'] = df['jump'].shift(-1)\n",
    "\n",
    "# feature engineering\n",
    "df = add_atr(df)\n",
    "df = add_ema(df)\n",
    "df = add_vwap(df)\n",
    "\n",
    "df = add_dow(df)\n",
    "df = pd.get_dummies(df, columns=['day_of_week'], prefix='dow', drop_first=True)\n",
    "df = df.dropna()\n",
    "\n",
    "# lag features\n",
    "lag_factor = 5\n",
    "cols = ['open', 'high', 'low', 'close', 'volume', 'atr', 'ema', 'VWAP']\n",
    "\n",
    "for lag in range(1, lag_factor+1):\n",
    "    for col in cols:\n",
    "        newcol = np.zeros(df.shape[0]) * np.nan\n",
    "        newcol[lag:] = df[col].values[:-lag]\n",
    "        df.insert(len(df.columns), \"{0}_{1}\".format(col, lag), newcol)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# move the jump and target variable (jump_tmr) to the end\n",
    "df = pd.get_dummies(df, columns=['jump'], prefix='jump', drop_first=True)\n",
    "df = df[[col for col in df.columns if col not in ['next_jump']] + ['next_jump']]\n",
    "\n",
    "for col, dtype in zip(df.columns, df.dtypes):\n",
    "    print(col, dtype)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num big down: 0\n",
      "num small down: 0\n",
      "num neutral: 27788\n",
      "num small up: 0\n",
      "num big up: 0\n",
      "num down 9598\n",
      "num up 6411\n"
     ]
    }
   ],
   "source": [
    "n_big_down = list(df['next_jump']).count('big_down')\n",
    "n_small_down = list(df['next_jump']).count('small_down')\n",
    "n_neutral = list(df['next_jump']).count('neutral')\n",
    "n_small_up = list(df['next_jump']).count('small_up')\n",
    "n_big_up = list(df['next_jump']).count('big_up')\n",
    "\n",
    "print('num big down:', n_big_down)\n",
    "print('num small down:', n_small_down)\n",
    "print('num neutral:', n_neutral)\n",
    "print('num small up:', n_small_up)\n",
    "print('num big up:', n_big_up)\n",
    "\n",
    "n_down = list(df['next_jump']).count('down')\n",
    "n_up = list(df['next_jump']).count('up')\n",
    "\n",
    "print('num down', n_down)\n",
    "print('num up', n_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost specific preprocessing\n",
    "\n",
    "xgbDF = df.copy()\n",
    "\n",
    "jump_lookup = {\n",
    "    'down':0,\n",
    "    'neutral':1,\n",
    "    'up':2\n",
    "}\n",
    "\n",
    "xgbDF['next_jump'] = xgbDF['next_jump'].map(jump_lookup)\n",
    "\n",
    "X = xgbDF.drop(['next_jump', 'date'], axis=1).copy()\n",
    "y = xgbDF['next_jump'].copy()\n",
    "m = xgb.DMatrix(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False) \n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ Classification Report ------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1701\n",
      "           1       0.69      1.00      0.82      6087\n",
      "           2       0.00      0.00      0.00       972\n",
      "\n",
      "    accuracy                           0.69      8760\n",
      "   macro avg       0.23      0.33      0.27      8760\n",
      "weighted avg       0.48      0.69      0.57      8760\n",
      "\n",
      "\n",
      "\n",
      "-------------- Confusion Matrix --------------\n",
      "[[   0 1701    0]\n",
      " [   0 6087    0]\n",
      " [   0  972    0]]\n"
     ]
    }
   ],
   "source": [
    "# baseline prediction with all set to neutral\n",
    "y_pred = np.zeros(X_test.shape[0])\n",
    "for idx in range(len(y_pred)):\n",
    "    y_pred[idx] = '1'\n",
    "\n",
    "classification_summary(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "XGBoost 1\n",
    "***\n",
    "\n",
    "Naive Implementation using custom weighted F1-score evaluation metric and softprob objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_Weighted\u001b[39m\u001b[38;5;124m'\u001b[39m, f1\n\u001b[0;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisable_default_eval_metric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftprob_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_weighted_eval\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:2107\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2107\u001b[0m     grad, hess \u001b[38;5;241m=\u001b[39m \u001b[43mfobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboost(dtrain, iteration\u001b[38;5;241m=\u001b[39miteration, grad\u001b[38;5;241m=\u001b[39mgrad, hess\u001b[38;5;241m=\u001b[39mhess)\n",
      "Cell \u001b[0;32mIn[17], line 40\u001b[0m, in \u001b[0;36msoftprob_obj\u001b[0;34m(predt, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(predt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m target \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m target \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m kClasses\n\u001b[0;32m---> 40\u001b[0m     g \u001b[38;5;241m=\u001b[39m p[c] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m==\u001b[39m target \u001b[38;5;28;01melse\u001b[39;00m p[c]\n\u001b[1;32m     41\u001b[0m     g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m*\u001b[39m weights[r]\n\u001b[1;32m     42\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m p[c] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m p[c]) \u001b[38;5;241m*\u001b[39m weights[r])\u001b[38;5;241m.\u001b[39mitem(), eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    '''Softmax function with x as input vector.'''\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def softprob_obj(predt: np.ndarray, data: xgb.DMatrix):\n",
    "    '''Loss function.  Computing the gradient and approximated hessian (diagonal).\n",
    "    Reimplements the `multi:softprob` inside XGBoost.\n",
    "\n",
    "    '''\n",
    "    labels = data.get_label()\n",
    "    kRows = predt.shape[0]\n",
    "    kClasses = 5\n",
    "\n",
    "    if data.get_weight().size == 0:\n",
    "        # Use 1 as weight if we don't have custom weight.\n",
    "        weights = np.ones((kRows, 1), dtype=float)\n",
    "    else:\n",
    "        weights = data.get_weight()\n",
    "\n",
    "    # The prediction is of shape (rows, classes), each element in a row\n",
    "    # represents a raw prediction (leaf weight, hasn't gone through softmax\n",
    "    # yet).\n",
    "    assert predt.shape == (kRows, kClasses)\n",
    "\n",
    "    grad = np.zeros((kRows, kClasses), dtype=float)\n",
    "    hess = np.zeros((kRows, kClasses), dtype=float)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # compute the gradient and hessian, slow iterations in Python, only\n",
    "    # suitable for demo.  Also the one in native XGBoost core is more robust to\n",
    "    # numeric overflow as we don't do anything to mitigate the `exp` in\n",
    "    # `softmax` here.\n",
    "    for r in range(predt.shape[0]):\n",
    "        target = labels[r]\n",
    "        p = softmax(predt[r, :])\n",
    "        for c in range(predt.shape[1]):\n",
    "            assert target >= 0 or target <= kClasses\n",
    "            g = p[c] - 1.0 if c == target else p[c]\n",
    "            g = g * weights[r]\n",
    "            h = max((2.0 * p[c] * (1.0 - p[c]) * weights[r]).item(), eps)\n",
    "            grad[r, c] = g\n",
    "            hess[r, c] = h\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "def f1_weighted_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y_true = dtrain.get_label()\n",
    "    # Convert raw logits to predicted class\n",
    "    y_pred = np.argmax(predt, axis=1)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_Weighted', f1\n",
    "\n",
    "model = xgb.train(\n",
    "    {\n",
    "        'num_class':5,\n",
    "        'disable_default_eval_metric':True\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=10,\n",
    "    obj=softprob_obj,\n",
    "    custom_metric=f1_weighted_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "\n",
      "------------ Classification Report ------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.02      0.04       728\n",
      "           1       0.89      0.99      0.94      7747\n",
      "           2       0.53      0.07      0.13       285\n",
      "\n",
      "    accuracy                           0.88      8760\n",
      "   macro avg       0.58      0.36      0.37      8760\n",
      "weighted avg       0.83      0.88      0.84      8760\n",
      "\n",
      "\n",
      "\n",
      "-------------- Confusion Matrix --------------\n",
      "[[  16  708    4]\n",
      " [  31 7701   15]\n",
      " [   4  260   21]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(dtest)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "classification_summary(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "XGBoost Model 2\n",
    "***\n",
    "\n",
    "Tuned using `Optuna`. See https://www.kaggle.com/code/para24/xgboost-stepwise-tuning-using-optuna/notebook#7.-Stepwise-Hyperparameter-Tuning\n",
    "\n",
    "We separate the 6 most effective and commonly altered XGBoost hyperparameters into 3 groups as follows:\n",
    "\n",
    "Group 1: `max_depth`, `min_child_weight`\n",
    "\n",
    "Group 2: `subsample`, `colsample_bytree`\n",
    "\n",
    "Group 3: `learning_rate`, `num_boost_round`\n",
    "\n",
    "We then tune each group sequentially, finding the optimal value for each group using previous findings (with `learning_rate` and `num_boost_round` set to some default values to begin with before being the final group to be optimised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['date', 'next_jump'], axis=1).copy()\n",
    "y = df['next_jump'].copy()\n",
    "y = y.map(jump_lookup)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study, logging\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "def objective(trial, X, y, group, score, params=dict()):\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "    # initial learning params\n",
    "    params['num_boost_round'] = 200\n",
    "    params['learning_rate'] = 0.01\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "\n",
    "    if group == '1':\n",
    "        params['max_depth'] = trial.suggest_int('max_depth', 2, 30)\n",
    "        params['min_child_weight'] = trial.suggest_loguniform('min_child_weight', 1e-10, 1e10)\n",
    "    \n",
    "    if group == '2':\n",
    "        params['subsample'] = trial.suggest_uniform('subsample', 0, 1)\n",
    "        params['colsample_bytree'] = trial.suggest_uniform('colsample_bytree', 0, 1)\n",
    "\n",
    "    if group == '3':\n",
    "        num_boost_round = trial.suggest_int('num_boost_round', 100, 400)\n",
    "        learning_rate = trial.suggest_uniform('learning_rate', 0.005, 0.1)\n",
    "        params['num_boost_round'] = num_boost_round\n",
    "        params['learning_rate'] = learning_rate\n",
    "\n",
    "    pruning_callback = XGBoostPruningCallback(trial, 'test-' + score.__name__)\n",
    "\n",
    "    cv_scores = xgb.cv(params, dtrain, nfold=5,\n",
    "                       stratified=True,\n",
    "                       feval=score,\n",
    "                       num_boost_round=params['num_boost_round'],\n",
    "                       early_stopping_rounds=10,\n",
    "                       callbacks=[pruning_callback])\n",
    "    \n",
    "    return cv_scores['test-' + score.__name__ + '-mean'].values[-1]\n",
    "\n",
    "def execute_optimization(study_name, group, score, trials, params=dict(), direction='maximize'):\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "    ## use pruner to skip trials that aren't doing so well\n",
    "    pruner = MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "    study = create_study(\n",
    "        direction=direction,\n",
    "        study_name=study_name,\n",
    "        storage='sqlite:///optuna.db',\n",
    "        load_if_exists=True,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, group, score, params),\n",
    "        n_trials=trials,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    print('STUDY NAME: ', study_name)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('EVALUATION METRIC: ', score.__name__)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('BEST CV SCORE: ', study.best_value)\n",
    "    print('-------------------------------------------------------')\n",
    "    print(f'OPTIMAL GROUP - {group} PARAMS: ', study.best_params)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('BEST TRIAL', study.best_trial)\n",
    "    print('-------------------------------------------------------')\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "def f1_weighted_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = np.argmax(predt, axis=1)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_score', f1\n",
    "\n",
    "f1_weighted_eval.__name__ = f1_score.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Optimizing Group 1 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.4302664\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 1 PARAMS:  {'max_depth': 15, 'min_child_weight': 0.004207887542004267}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=400, state=TrialState.COMPLETE, values=[0.4302664], datetime_start=datetime.datetime(2025, 1, 15, 15, 21, 26, 303993), datetime_complete=datetime.datetime(2025, 1, 15, 15, 22, 3, 710586), params={'max_depth': 15, 'min_child_weight': 0.004207887542004267}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4302664, 1: 0.4314996, 2: 0.43100700000000003, 3: 0.43124019999999996, 4: 0.43188640000000006, 5: 0.4315086, 6: 0.4317665999999999, 7: 0.4313644, 8: 0.4314776, 9: 0.4321339999999999}, distributions={'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_child_weight': FloatDistribution(high=10000000000.0, log=True, low=1e-10, step=None)}, trial_id=401, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 1:  {'num_boost_round': 200, 'learning_rate': 0.01, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 15, 'min_child_weight': 0.004207887542004267}\n",
      "\n",
      "\n",
      "\n",
      "====== Optimizing Group 2 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.4326498\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 2 PARAMS:  {'subsample': 0.5375453363150569, 'colsample_bytree': 0.6064743051193885}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=414, state=TrialState.COMPLETE, values=[0.4326498], datetime_start=datetime.datetime(2025, 1, 15, 15, 27, 54, 982441), datetime_complete=datetime.datetime(2025, 1, 15, 15, 28, 13, 512014), params={'subsample': 0.5375453363150569, 'colsample_bytree': 0.6064743051193885}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4326498, 1: 0.4410386, 2: 0.44040280000000004, 3: 0.4411564, 4: 0.44061580000000006, 5: 0.43930220000000003, 6: 0.4377084, 7: 0.4371034, 8: 0.4375844, 9: 0.43691060000000004}, distributions={'subsample': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'colsample_bytree': FloatDistribution(high=1.0, log=False, low=0.0, step=None)}, trial_id=415, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 2:  {'num_boost_round': 200, 'learning_rate': 0.01, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 15, 'min_child_weight': 0.004207887542004267, 'subsample': 0.5375453363150569, 'colsample_bytree': 0.6064743051193885}\n",
      "\n",
      "\n",
      "\n",
      "====== Optimizing Group 3 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.43266659999999996\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 3 PARAMS:  {'num_boost_round': 299, 'learning_rate': 0.06342062859951908}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=428, state=TrialState.COMPLETE, values=[0.43266659999999996], datetime_start=datetime.datetime(2025, 1, 15, 15, 32, 50, 389488), datetime_complete=datetime.datetime(2025, 1, 15, 15, 33, 8, 214737), params={'num_boost_round': 299, 'learning_rate': 0.06342062859951908}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.43266659999999996, 1: 0.44097159999999996, 2: 0.44109500000000007, 3: 0.44161740000000005, 4: 0.44302400000000003, 5: 0.4407434, 6: 0.43943580000000004, 7: 0.43727479999999996, 8: 0.4344754, 9: 0.4328798000000001}, distributions={'num_boost_round': IntDistribution(high=400, log=False, low=100, step=1), 'learning_rate': FloatDistribution(high=0.1, log=False, low=0.005, step=None)}, trial_id=429, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 3:  {'num_boost_round': 299, 'learning_rate': 0.06342062859951908, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 15, 'min_child_weight': 0.004207887542004267, 'subsample': 0.5375453363150569, 'colsample_bytree': 0.6064743051193885}\n",
      "\n",
      "\n",
      "\n",
      "====== Final Optimal Parameters ======\n",
      "{'num_boost_round': 299, 'learning_rate': 0.06342062859951908, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 15, 'min_child_weight': 0.004207887542004267, 'subsample': 0.5375453363150569, 'colsample_bytree': 0.6064743051193885}\n"
     ]
    }
   ],
   "source": [
    "def stepwise_optimization(trials=10):\n",
    "    final_params = dict()\n",
    "    for g in ['1', '2', '3']:\n",
    "        print(f'====== Optimizing Group {g} ======')\n",
    "        update_params = execute_optimization(\n",
    "            'xgboost', g, f1_weighted_eval, trials, params=final_params, direction='maximize'\n",
    "        )\n",
    "        final_params.update(update_params)\n",
    "        print(f'Params after updating group {g}: ', final_params)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    print(f'====== Final Optimal Parameters ======')\n",
    "    print(final_params)\n",
    "\n",
    "    return final_params\n",
    "\n",
    "xgb2_params = stepwise_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0xff3d5d817070>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2_params['num_class'] = 3\n",
    "\n",
    "model = xgb.train(\n",
    "    params=xgb2_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=xgb2_params['num_boost_round']\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "\n",
      "----------------- classification report ----------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.08      0.14      1701\n",
      "           1       0.71      0.97      0.82      6087\n",
      "           2       0.53      0.06      0.11       972\n",
      "\n",
      "    accuracy                           0.70      8760\n",
      "   macro avg       0.55      0.37      0.36      8760\n",
      "weighted avg       0.64      0.70      0.61      8760\n",
      "\n",
      "\n",
      "------------------- confusion matrix -------------------\n",
      "\n",
      "[[ 142 1539   20]\n",
      " [ 134 5920   33]\n",
      " [  62  851   59]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(xgb.DMatrix(X_test, label=y_test))\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print('\\n----------------- classification report ----------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('\\n------------------- confusion matrix -------------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
