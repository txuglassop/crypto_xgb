{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build upon findings of the `xgboost` model in `classification_models.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path(\"..\", \"src\")\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "from feature_engineering import add_vwap, add_atr, add_ema, add_dow, add_return, add_jump_categories_3, add_jump_categories_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do all the data cleaning / feature engineering and `xgboost` specific preprocessing as seen in the aforementioned notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43817 entries, 0 to 43816\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   date                      43817 non-null  float64\n",
      " 1   open                      43817 non-null  float64\n",
      " 2   high                      43817 non-null  float64\n",
      " 3   low                       43817 non-null  float64\n",
      " 4   close                     43817 non-null  float64\n",
      " 5   volume                    43817 non-null  float64\n",
      " 6   base_asset_volume         43817 non-null  float64\n",
      " 7   no_trades                 43817 non-null  int64  \n",
      " 8   taker_buy_vol             43817 non-null  float64\n",
      " 9   taker_buy_base_asset_vol  43817 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.3 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>base_asset_volume</th>\n",
       "      <th>no_trades</th>\n",
       "      <th>taker_buy_vol</th>\n",
       "      <th>taker_buy_base_asset_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>129.16</td>\n",
       "      <td>129.19</td>\n",
       "      <td>128.68</td>\n",
       "      <td>128.87</td>\n",
       "      <td>7769.17336</td>\n",
       "      <td>1.000930e+06</td>\n",
       "      <td>2504</td>\n",
       "      <td>4149.93345</td>\n",
       "      <td>534619.3390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>128.87</td>\n",
       "      <td>130.65</td>\n",
       "      <td>128.78</td>\n",
       "      <td>130.64</td>\n",
       "      <td>11344.65516</td>\n",
       "      <td>1.474278e+06</td>\n",
       "      <td>4885</td>\n",
       "      <td>5930.54276</td>\n",
       "      <td>770486.0567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.577840e+12</td>\n",
       "      <td>130.63</td>\n",
       "      <td>130.98</td>\n",
       "      <td>130.35</td>\n",
       "      <td>130.85</td>\n",
       "      <td>7603.35623</td>\n",
       "      <td>9.940256e+05</td>\n",
       "      <td>3046</td>\n",
       "      <td>3324.35218</td>\n",
       "      <td>434675.4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.577850e+12</td>\n",
       "      <td>130.85</td>\n",
       "      <td>130.89</td>\n",
       "      <td>129.94</td>\n",
       "      <td>130.20</td>\n",
       "      <td>4968.55433</td>\n",
       "      <td>6.473610e+05</td>\n",
       "      <td>2818</td>\n",
       "      <td>1810.03564</td>\n",
       "      <td>235890.3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.577850e+12</td>\n",
       "      <td>130.21</td>\n",
       "      <td>130.74</td>\n",
       "      <td>130.15</td>\n",
       "      <td>130.20</td>\n",
       "      <td>3397.90747</td>\n",
       "      <td>4.430067e+05</td>\n",
       "      <td>2264</td>\n",
       "      <td>1839.74371</td>\n",
       "      <td>239848.3483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    open    high     low   close       volume  \\\n",
       "0  1.577840e+12  129.16  129.19  128.68  128.87   7769.17336   \n",
       "1  1.577840e+12  128.87  130.65  128.78  130.64  11344.65516   \n",
       "2  1.577840e+12  130.63  130.98  130.35  130.85   7603.35623   \n",
       "3  1.577850e+12  130.85  130.89  129.94  130.20   4968.55433   \n",
       "4  1.577850e+12  130.21  130.74  130.15  130.20   3397.90747   \n",
       "\n",
       "   base_asset_volume  no_trades  taker_buy_vol  taker_buy_base_asset_vol  \n",
       "0       1.000930e+06       2504     4149.93345               534619.3390  \n",
       "1       1.474278e+06       4885     5930.54276               770486.0567  \n",
       "2       9.940256e+05       3046     3324.35218               434675.4447  \n",
       "3       6.473610e+05       2818     1810.03564               235890.3302  \n",
       "4       4.430067e+05       2264     1839.74371               239848.3483  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"./../input/ETHUSDT_1h_2020_2024_join_final.csv\")\n",
    "\n",
    "df_raw.drop(df_raw.columns[df_raw.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "print(df_raw.info())\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date datetime64[ns]\n",
      "open float64\n",
      "high float64\n",
      "low float64\n",
      "close float64\n",
      "volume float64\n",
      "base_asset_volume float64\n",
      "no_trades int64\n",
      "taker_buy_vol float64\n",
      "taker_buy_base_asset_vol float64\n",
      "return float64\n",
      "atr float64\n",
      "ema float64\n",
      "VWAP float64\n",
      "dow_Monday bool\n",
      "dow_Saturday bool\n",
      "dow_Sunday bool\n",
      "dow_Thursday bool\n",
      "dow_Tuesday bool\n",
      "dow_Wednesday bool\n",
      "open_1 float64\n",
      "high_1 float64\n",
      "low_1 float64\n",
      "close_1 float64\n",
      "volume_1 float64\n",
      "atr_1 float64\n",
      "ema_1 float64\n",
      "VWAP_1 float64\n",
      "open_2 float64\n",
      "high_2 float64\n",
      "low_2 float64\n",
      "close_2 float64\n",
      "volume_2 float64\n",
      "atr_2 float64\n",
      "ema_2 float64\n",
      "VWAP_2 float64\n",
      "open_3 float64\n",
      "high_3 float64\n",
      "low_3 float64\n",
      "close_3 float64\n",
      "volume_3 float64\n",
      "atr_3 float64\n",
      "ema_3 float64\n",
      "VWAP_3 float64\n",
      "open_4 float64\n",
      "high_4 float64\n",
      "low_4 float64\n",
      "close_4 float64\n",
      "volume_4 float64\n",
      "atr_4 float64\n",
      "ema_4 float64\n",
      "VWAP_4 float64\n",
      "open_5 float64\n",
      "high_5 float64\n",
      "low_5 float64\n",
      "close_5 float64\n",
      "volume_5 float64\n",
      "atr_5 float64\n",
      "ema_5 float64\n",
      "VWAP_5 float64\n",
      "jump_neutral bool\n",
      "jump_up bool\n",
      "next_jump object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>base_asset_volume</th>\n",
       "      <th>no_trades</th>\n",
       "      <th>taker_buy_vol</th>\n",
       "      <th>taker_buy_base_asset_vol</th>\n",
       "      <th>...</th>\n",
       "      <th>high_5</th>\n",
       "      <th>low_5</th>\n",
       "      <th>close_5</th>\n",
       "      <th>volume_5</th>\n",
       "      <th>atr_5</th>\n",
       "      <th>ema_5</th>\n",
       "      <th>VWAP_5</th>\n",
       "      <th>jump_neutral</th>\n",
       "      <th>jump_up</th>\n",
       "      <th>next_jump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>132.04</td>\n",
       "      <td>132.16</td>\n",
       "      <td>131.62</td>\n",
       "      <td>131.86</td>\n",
       "      <td>2111.21443</td>\n",
       "      <td>2.783557e+05</td>\n",
       "      <td>1995</td>\n",
       "      <td>997.52946</td>\n",
       "      <td>1.315025e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.24</td>\n",
       "      <td>131.96</td>\n",
       "      <td>7325.25762</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>131.029332</td>\n",
       "      <td>130.561825</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>131.86</td>\n",
       "      <td>132.25</td>\n",
       "      <td>131.70</td>\n",
       "      <td>132.18</td>\n",
       "      <td>2014.79285</td>\n",
       "      <td>2.660484e+05</td>\n",
       "      <td>1988</td>\n",
       "      <td>1021.42474</td>\n",
       "      <td>1.349028e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.40</td>\n",
       "      <td>131.60</td>\n",
       "      <td>132.08</td>\n",
       "      <td>5361.06926</td>\n",
       "      <td>0.113214</td>\n",
       "      <td>131.228799</td>\n",
       "      <td>130.641168</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>132.17</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.68</td>\n",
       "      <td>131.78</td>\n",
       "      <td>4879.42025</td>\n",
       "      <td>6.440060e+05</td>\n",
       "      <td>2410</td>\n",
       "      <td>1841.37772</td>\n",
       "      <td>2.429792e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.95</td>\n",
       "      <td>131.78</td>\n",
       "      <td>132.85</td>\n",
       "      <td>6915.20906</td>\n",
       "      <td>0.091658</td>\n",
       "      <td>131.488372</td>\n",
       "      <td>130.764300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>131.82</td>\n",
       "      <td>131.82</td>\n",
       "      <td>129.90</td>\n",
       "      <td>130.27</td>\n",
       "      <td>14876.06749</td>\n",
       "      <td>1.943372e+06</td>\n",
       "      <td>6386</td>\n",
       "      <td>5520.77235</td>\n",
       "      <td>7.209741e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>133.05</td>\n",
       "      <td>132.27</td>\n",
       "      <td>132.34</td>\n",
       "      <td>5424.00732</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>131.701365</td>\n",
       "      <td>130.851473</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>130.28</td>\n",
       "      <td>130.87</td>\n",
       "      <td>129.74</td>\n",
       "      <td>130.77</td>\n",
       "      <td>3865.45991</td>\n",
       "      <td>5.035546e+05</td>\n",
       "      <td>3232</td>\n",
       "      <td>2025.11315</td>\n",
       "      <td>2.639359e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.46</td>\n",
       "      <td>131.57</td>\n",
       "      <td>132.04</td>\n",
       "      <td>5707.79340</td>\n",
       "      <td>0.068019</td>\n",
       "      <td>131.765758</td>\n",
       "      <td>130.908630</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>130.72</td>\n",
       "      <td>130.78</td>\n",
       "      <td>130.27</td>\n",
       "      <td>130.67</td>\n",
       "      <td>3772.66670</td>\n",
       "      <td>4.925267e+05</td>\n",
       "      <td>2565</td>\n",
       "      <td>2094.53022</td>\n",
       "      <td>2.734273e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.16</td>\n",
       "      <td>131.62</td>\n",
       "      <td>131.86</td>\n",
       "      <td>2111.21443</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>131.788607</td>\n",
       "      <td>130.925844</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>130.66</td>\n",
       "      <td>130.67</td>\n",
       "      <td>130.12</td>\n",
       "      <td>130.15</td>\n",
       "      <td>3684.51912</td>\n",
       "      <td>4.803441e+05</td>\n",
       "      <td>2414</td>\n",
       "      <td>1879.43297</td>\n",
       "      <td>2.450120e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>132.25</td>\n",
       "      <td>131.70</td>\n",
       "      <td>132.18</td>\n",
       "      <td>2014.79285</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>131.839552</td>\n",
       "      <td>130.944429</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>130.14</td>\n",
       "      <td>130.16</td>\n",
       "      <td>128.89</td>\n",
       "      <td>129.72</td>\n",
       "      <td>19078.42209</td>\n",
       "      <td>2.469243e+06</td>\n",
       "      <td>8599</td>\n",
       "      <td>10251.27762</td>\n",
       "      <td>1.326941e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>132.37</td>\n",
       "      <td>131.68</td>\n",
       "      <td>131.78</td>\n",
       "      <td>4879.42025</td>\n",
       "      <td>0.052313</td>\n",
       "      <td>131.860308</td>\n",
       "      <td>130.983103</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>129.71</td>\n",
       "      <td>129.71</td>\n",
       "      <td>128.77</td>\n",
       "      <td>129.10</td>\n",
       "      <td>11950.18634</td>\n",
       "      <td>1.544526e+06</td>\n",
       "      <td>5294</td>\n",
       "      <td>6776.08848</td>\n",
       "      <td>8.759077e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>131.82</td>\n",
       "      <td>129.90</td>\n",
       "      <td>130.27</td>\n",
       "      <td>14876.06749</td>\n",
       "      <td>0.140880</td>\n",
       "      <td>131.620913</td>\n",
       "      <td>130.949343</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>129.09</td>\n",
       "      <td>129.87</td>\n",
       "      <td>128.69</td>\n",
       "      <td>129.55</td>\n",
       "      <td>8931.67759</td>\n",
       "      <td>1.156161e+06</td>\n",
       "      <td>4813</td>\n",
       "      <td>4789.83655</td>\n",
       "      <td>6.200412e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>130.87</td>\n",
       "      <td>129.74</td>\n",
       "      <td>130.77</td>\n",
       "      <td>3865.45991</td>\n",
       "      <td>0.090777</td>\n",
       "      <td>131.388731</td>\n",
       "      <td>130.936277</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    open    high     low   close       volume  base_asset_volume  \\\n",
       "19 2020-01-01  132.04  132.16  131.62  131.86   2111.21443       2.783557e+05   \n",
       "20 2020-01-01  131.86  132.25  131.70  132.18   2014.79285       2.660484e+05   \n",
       "21 2020-01-01  132.17  132.37  131.68  131.78   4879.42025       6.440060e+05   \n",
       "22 2020-01-01  131.82  131.82  129.90  130.27  14876.06749       1.943372e+06   \n",
       "23 2020-01-01  130.28  130.87  129.74  130.77   3865.45991       5.035546e+05   \n",
       "24 2020-01-01  130.72  130.78  130.27  130.67   3772.66670       4.925267e+05   \n",
       "25 2020-01-02  130.66  130.67  130.12  130.15   3684.51912       4.803441e+05   \n",
       "26 2020-01-02  130.14  130.16  128.89  129.72  19078.42209       2.469243e+06   \n",
       "27 2020-01-02  129.71  129.71  128.77  129.10  11950.18634       1.544526e+06   \n",
       "28 2020-01-02  129.09  129.87  128.69  129.55   8931.67759       1.156161e+06   \n",
       "\n",
       "    no_trades  taker_buy_vol  taker_buy_base_asset_vol  ...  high_5   low_5  \\\n",
       "19       1995      997.52946              1.315025e+05  ...  132.37  131.24   \n",
       "20       1988     1021.42474              1.349028e+05  ...  132.40  131.60   \n",
       "21       2410     1841.37772              2.429792e+05  ...  132.95  131.78   \n",
       "22       6386     5520.77235              7.209741e+05  ...  133.05  132.27   \n",
       "23       3232     2025.11315              2.639359e+05  ...  132.46  131.57   \n",
       "24       2565     2094.53022              2.734273e+05  ...  132.16  131.62   \n",
       "25       2414     1879.43297              2.450120e+05  ...  132.25  131.70   \n",
       "26       8599    10251.27762              1.326941e+06  ...  132.37  131.68   \n",
       "27       5294     6776.08848              8.759077e+05  ...  131.82  129.90   \n",
       "28       4813     4789.83655              6.200412e+05  ...  130.87  129.74   \n",
       "\n",
       "    close_5     volume_5     atr_5       ema_5      VWAP_5  jump_neutral  \\\n",
       "19   131.96   7325.25762  0.785000  131.029332  130.561825          True   \n",
       "20   132.08   5361.06926  0.113214  131.228799  130.641168          True   \n",
       "21   132.85   6915.20906  0.091658  131.488372  130.764300          True   \n",
       "22   132.34   5424.00732  0.062261  131.701365  130.851473         False   \n",
       "23   132.04   5707.79340  0.068019  131.765758  130.908630          True   \n",
       "24   131.86   2111.21443  0.043430  131.788607  130.925844          True   \n",
       "25   132.18   2014.79285  0.042388  131.839552  130.944429          True   \n",
       "26   131.78   4879.42025  0.052313  131.860308  130.983103          True   \n",
       "27   130.27  14876.06749  0.140880  131.620913  130.949343          True   \n",
       "28   130.77   3865.45991  0.090777  131.388731  130.936277          True   \n",
       "\n",
       "    jump_up  next_jump  \n",
       "19    False    neutral  \n",
       "20    False    neutral  \n",
       "21    False       down  \n",
       "22    False    neutral  \n",
       "23    False    neutral  \n",
       "24    False    neutral  \n",
       "25    False    neutral  \n",
       "26    False    neutral  \n",
       "27    False    neutral  \n",
       "28    False    neutral  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general data cleaning\n",
    "df = df_raw.copy()\n",
    "\n",
    "df = add_return(df)\n",
    "\n",
    "# add jump feature and target variable\n",
    "df = add_jump_categories_3(df, margin=0.008)\n",
    "df['next_jump'] = df['jump'].shift(-1)\n",
    "\n",
    "# feature engineering\n",
    "df = add_atr(df)\n",
    "df = add_ema(df)\n",
    "df = add_vwap(df)\n",
    "\n",
    "df = add_dow(df)\n",
    "df = pd.get_dummies(df, columns=['day_of_week'], prefix='dow', drop_first=True)\n",
    "df = df.dropna()\n",
    "\n",
    "# lag features\n",
    "lag_factor = 5\n",
    "cols = ['open', 'high', 'low', 'close', 'volume', 'atr', 'ema', 'VWAP']\n",
    "\n",
    "for lag in range(1, lag_factor+1):\n",
    "    for col in cols:\n",
    "        newcol = np.zeros(df.shape[0]) * np.nan\n",
    "        newcol[lag:] = df[col].values[:-lag]\n",
    "        df.insert(len(df.columns), \"{0}_{1}\".format(col, lag), newcol)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# move the jump and target variable (jump_tmr) to the end\n",
    "df = pd.get_dummies(df, columns=['jump'], prefix='jump', drop_first=True)\n",
    "df = df[[col for col in df.columns if col not in ['next_jump']] + ['next_jump']]\n",
    "\n",
    "for col, dtype in zip(df.columns, df.dtypes):\n",
    "    print(col, dtype)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num big down: 0\n",
      "num small down: 0\n",
      "num neutral: 37197\n",
      "num small up: 0\n",
      "num big up: 0\n",
      "num down 3242\n",
      "num up 3358\n"
     ]
    }
   ],
   "source": [
    "n_big_down = list(df['next_jump']).count('big_down')\n",
    "n_small_down = list(df['next_jump']).count('small_down')\n",
    "n_neutral = list(df['next_jump']).count('neutral')\n",
    "n_small_up = list(df['next_jump']).count('small_up')\n",
    "n_big_up = list(df['next_jump']).count('big_up')\n",
    "\n",
    "print('num big down:', n_big_down)\n",
    "print('num small down:', n_small_down)\n",
    "print('num neutral:', n_neutral)\n",
    "print('num small up:', n_small_up)\n",
    "print('num big up:', n_big_up)\n",
    "\n",
    "n_down = list(df['next_jump']).count('down')\n",
    "n_up = list(df['next_jump']).count('up')\n",
    "\n",
    "print('num down', n_down)\n",
    "print('num up', n_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost specific preprocessing\n",
    "\n",
    "xgbDF = df.copy()\n",
    "\n",
    "jump_lookup = {\n",
    "    'down':0,\n",
    "    'neutral':1,\n",
    "    'up':2\n",
    "}\n",
    "\n",
    "xgbDF['next_jump'] = xgbDF['next_jump'].map(jump_lookup)\n",
    "\n",
    "X = xgbDF.drop(['next_jump', 'date'], axis=1).copy()\n",
    "y = xgbDF['next_jump'].copy()\n",
    "m = xgb.DMatrix(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False) \n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       443\n",
      "           1       0.90      1.00      0.95      7891\n",
      "           2       0.00      0.00      0.00       426\n",
      "\n",
      "    accuracy                           0.90      8760\n",
      "   macro avg       0.30      0.33      0.32      8760\n",
      "weighted avg       0.81      0.90      0.85      8760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline prediction with all set to neutral\n",
    "y_pred = np.zeros(X_test.shape[0])\n",
    "for idx in range(len(y_pred)):\n",
    "    y_pred[idx] = '1'\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "XGBoost 1\n",
    "***\n",
    "\n",
    "Naive Implementation using custom weighted F1-score evaluation metric and softprob objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-F1_Weighted:0.42873\n",
      "[1]\ttrain-F1_Weighted:0.42598\n",
      "[2]\ttrain-F1_Weighted:0.42485\n",
      "[3]\ttrain-F1_Weighted:0.42969\n",
      "[4]\ttrain-F1_Weighted:0.43270\n",
      "[5]\ttrain-F1_Weighted:0.43359\n",
      "[6]\ttrain-F1_Weighted:0.43644\n",
      "[7]\ttrain-F1_Weighted:0.43920\n",
      "[8]\ttrain-F1_Weighted:0.43887\n",
      "[9]\ttrain-F1_Weighted:0.44238\n",
      "[10]\ttrain-F1_Weighted:0.44388\n",
      "[11]\ttrain-F1_Weighted:0.44584\n",
      "[12]\ttrain-F1_Weighted:0.44734\n",
      "[13]\ttrain-F1_Weighted:0.44856\n",
      "[14]\ttrain-F1_Weighted:0.45254\n",
      "[15]\ttrain-F1_Weighted:0.45373\n",
      "[16]\ttrain-F1_Weighted:0.45674\n",
      "[17]\ttrain-F1_Weighted:0.46037\n",
      "[18]\ttrain-F1_Weighted:0.46451\n",
      "[19]\ttrain-F1_Weighted:0.46748\n",
      "[20]\ttrain-F1_Weighted:0.46870\n",
      "[21]\ttrain-F1_Weighted:0.47042\n",
      "[22]\ttrain-F1_Weighted:0.47393\n",
      "[23]\ttrain-F1_Weighted:0.47971\n",
      "[24]\ttrain-F1_Weighted:0.48133\n",
      "[25]\ttrain-F1_Weighted:0.48568\n",
      "[26]\ttrain-F1_Weighted:0.48815\n",
      "[27]\ttrain-F1_Weighted:0.49022\n",
      "[28]\ttrain-F1_Weighted:0.49357\n",
      "[29]\ttrain-F1_Weighted:0.49660\n",
      "[30]\ttrain-F1_Weighted:0.49844\n",
      "[31]\ttrain-F1_Weighted:0.50132\n",
      "[32]\ttrain-F1_Weighted:0.50435\n",
      "[33]\ttrain-F1_Weighted:0.50798\n",
      "[34]\ttrain-F1_Weighted:0.51015\n",
      "[35]\ttrain-F1_Weighted:0.51205\n",
      "[36]\ttrain-F1_Weighted:0.51465\n",
      "[37]\ttrain-F1_Weighted:0.51779\n",
      "[38]\ttrain-F1_Weighted:0.52255\n",
      "[39]\ttrain-F1_Weighted:0.52374\n",
      "[40]\ttrain-F1_Weighted:0.52496\n",
      "[41]\ttrain-F1_Weighted:0.52763\n",
      "[42]\ttrain-F1_Weighted:0.53026\n",
      "[43]\ttrain-F1_Weighted:0.53484\n",
      "[44]\ttrain-F1_Weighted:0.53641\n",
      "[45]\ttrain-F1_Weighted:0.53856\n",
      "[46]\ttrain-F1_Weighted:0.54026\n",
      "[47]\ttrain-F1_Weighted:0.54554\n",
      "[48]\ttrain-F1_Weighted:0.54757\n",
      "[49]\ttrain-F1_Weighted:0.54944\n",
      "[50]\ttrain-F1_Weighted:0.55317\n",
      "[51]\ttrain-F1_Weighted:0.55580\n",
      "[52]\ttrain-F1_Weighted:0.55766\n",
      "[53]\ttrain-F1_Weighted:0.55924\n",
      "[54]\ttrain-F1_Weighted:0.56383\n",
      "[55]\ttrain-F1_Weighted:0.56678\n",
      "[56]\ttrain-F1_Weighted:0.57144\n",
      "[57]\ttrain-F1_Weighted:0.57533\n",
      "[58]\ttrain-F1_Weighted:0.57655\n",
      "[59]\ttrain-F1_Weighted:0.57812\n",
      "[60]\ttrain-F1_Weighted:0.58017\n",
      "[61]\ttrain-F1_Weighted:0.58354\n",
      "[62]\ttrain-F1_Weighted:0.58579\n",
      "[63]\ttrain-F1_Weighted:0.58878\n",
      "[64]\ttrain-F1_Weighted:0.59277\n",
      "[65]\ttrain-F1_Weighted:0.59520\n",
      "[66]\ttrain-F1_Weighted:0.59762\n",
      "[67]\ttrain-F1_Weighted:0.60126\n",
      "[68]\ttrain-F1_Weighted:0.60242\n",
      "[69]\ttrain-F1_Weighted:0.60500\n",
      "[70]\ttrain-F1_Weighted:0.60894\n",
      "[71]\ttrain-F1_Weighted:0.61200\n",
      "[72]\ttrain-F1_Weighted:0.61427\n",
      "[73]\ttrain-F1_Weighted:0.61742\n",
      "[74]\ttrain-F1_Weighted:0.61840\n",
      "[75]\ttrain-F1_Weighted:0.62124\n",
      "[76]\ttrain-F1_Weighted:0.62442\n",
      "[77]\ttrain-F1_Weighted:0.62740\n",
      "[78]\ttrain-F1_Weighted:0.63327\n",
      "[79]\ttrain-F1_Weighted:0.63555\n",
      "[80]\ttrain-F1_Weighted:0.63925\n",
      "[81]\ttrain-F1_Weighted:0.64195\n",
      "[82]\ttrain-F1_Weighted:0.64382\n",
      "[83]\ttrain-F1_Weighted:0.64575\n",
      "[84]\ttrain-F1_Weighted:0.64726\n",
      "[85]\ttrain-F1_Weighted:0.64955\n",
      "[86]\ttrain-F1_Weighted:0.65133\n",
      "[87]\ttrain-F1_Weighted:0.65516\n",
      "[88]\ttrain-F1_Weighted:0.65691\n",
      "[89]\ttrain-F1_Weighted:0.65957\n",
      "[90]\ttrain-F1_Weighted:0.66171\n",
      "[91]\ttrain-F1_Weighted:0.66349\n",
      "[92]\ttrain-F1_Weighted:0.66618\n",
      "[93]\ttrain-F1_Weighted:0.66878\n",
      "[94]\ttrain-F1_Weighted:0.67014\n",
      "[95]\ttrain-F1_Weighted:0.67106\n",
      "[96]\ttrain-F1_Weighted:0.67320\n",
      "[97]\ttrain-F1_Weighted:0.67555\n",
      "[98]\ttrain-F1_Weighted:0.67743\n",
      "[99]\ttrain-F1_Weighted:0.67964\n",
      "[100]\ttrain-F1_Weighted:0.68210\n",
      "[101]\ttrain-F1_Weighted:0.68413\n",
      "[102]\ttrain-F1_Weighted:0.68508\n",
      "[103]\ttrain-F1_Weighted:0.68603\n",
      "[104]\ttrain-F1_Weighted:0.68882\n",
      "[105]\ttrain-F1_Weighted:0.69112\n",
      "[106]\ttrain-F1_Weighted:0.69290\n",
      "[107]\ttrain-F1_Weighted:0.69565\n",
      "[108]\ttrain-F1_Weighted:0.69871\n",
      "[109]\ttrain-F1_Weighted:0.69931\n",
      "[110]\ttrain-F1_Weighted:0.70098\n",
      "[111]\ttrain-F1_Weighted:0.70388\n",
      "[112]\ttrain-F1_Weighted:0.70547\n",
      "[113]\ttrain-F1_Weighted:0.70782\n",
      "[114]\ttrain-F1_Weighted:0.70949\n",
      "[115]\ttrain-F1_Weighted:0.71249\n",
      "[116]\ttrain-F1_Weighted:0.71267\n",
      "[117]\ttrain-F1_Weighted:0.71530\n",
      "[118]\ttrain-F1_Weighted:0.71815\n",
      "[119]\ttrain-F1_Weighted:0.72160\n",
      "[120]\ttrain-F1_Weighted:0.72229\n",
      "[121]\ttrain-F1_Weighted:0.72410\n",
      "[122]\ttrain-F1_Weighted:0.72667\n",
      "[123]\ttrain-F1_Weighted:0.73071\n",
      "[124]\ttrain-F1_Weighted:0.73394\n",
      "[125]\ttrain-F1_Weighted:0.73630\n",
      "[126]\ttrain-F1_Weighted:0.73787\n",
      "[127]\ttrain-F1_Weighted:0.73919\n",
      "[128]\ttrain-F1_Weighted:0.73994\n",
      "[129]\ttrain-F1_Weighted:0.74227\n",
      "[130]\ttrain-F1_Weighted:0.74453\n",
      "[131]\ttrain-F1_Weighted:0.74604\n",
      "[132]\ttrain-F1_Weighted:0.74787\n",
      "[133]\ttrain-F1_Weighted:0.74876\n",
      "[134]\ttrain-F1_Weighted:0.74995\n",
      "[135]\ttrain-F1_Weighted:0.75249\n",
      "[136]\ttrain-F1_Weighted:0.75339\n",
      "[137]\ttrain-F1_Weighted:0.75569\n",
      "[138]\ttrain-F1_Weighted:0.75870\n",
      "[139]\ttrain-F1_Weighted:0.76070\n",
      "[140]\ttrain-F1_Weighted:0.76379\n",
      "[141]\ttrain-F1_Weighted:0.76536\n",
      "[142]\ttrain-F1_Weighted:0.76870\n",
      "[143]\ttrain-F1_Weighted:0.77092\n",
      "[144]\ttrain-F1_Weighted:0.77339\n",
      "[145]\ttrain-F1_Weighted:0.77516\n",
      "[146]\ttrain-F1_Weighted:0.77820\n",
      "[147]\ttrain-F1_Weighted:0.77993\n",
      "[148]\ttrain-F1_Weighted:0.78169\n",
      "[149]\ttrain-F1_Weighted:0.78317\n",
      "[150]\ttrain-F1_Weighted:0.78542\n",
      "[151]\ttrain-F1_Weighted:0.78628\n",
      "[152]\ttrain-F1_Weighted:0.78838\n",
      "[153]\ttrain-F1_Weighted:0.79049\n",
      "[154]\ttrain-F1_Weighted:0.79273\n",
      "[155]\ttrain-F1_Weighted:0.79444\n",
      "[156]\ttrain-F1_Weighted:0.79615\n",
      "[157]\ttrain-F1_Weighted:0.79785\n",
      "[158]\ttrain-F1_Weighted:0.79952\n",
      "[159]\ttrain-F1_Weighted:0.80215\n",
      "[160]\ttrain-F1_Weighted:0.80434\n",
      "[161]\ttrain-F1_Weighted:0.80609\n",
      "[162]\ttrain-F1_Weighted:0.80724\n",
      "[163]\ttrain-F1_Weighted:0.80832\n",
      "[164]\ttrain-F1_Weighted:0.81085\n",
      "[165]\ttrain-F1_Weighted:0.81252\n",
      "[166]\ttrain-F1_Weighted:0.81399\n",
      "[167]\ttrain-F1_Weighted:0.81604\n",
      "[168]\ttrain-F1_Weighted:0.81736\n",
      "[169]\ttrain-F1_Weighted:0.81795\n",
      "[170]\ttrain-F1_Weighted:0.82024\n",
      "[171]\ttrain-F1_Weighted:0.82208\n",
      "[172]\ttrain-F1_Weighted:0.82287\n",
      "[173]\ttrain-F1_Weighted:0.82388\n",
      "[174]\ttrain-F1_Weighted:0.82490\n",
      "[175]\ttrain-F1_Weighted:0.82632\n",
      "[176]\ttrain-F1_Weighted:0.82702\n",
      "[177]\ttrain-F1_Weighted:0.82917\n",
      "[178]\ttrain-F1_Weighted:0.83118\n",
      "[179]\ttrain-F1_Weighted:0.83285\n",
      "[180]\ttrain-F1_Weighted:0.83365\n",
      "[181]\ttrain-F1_Weighted:0.83535\n",
      "[182]\ttrain-F1_Weighted:0.83722\n",
      "[183]\ttrain-F1_Weighted:0.83831\n",
      "[184]\ttrain-F1_Weighted:0.83896\n",
      "[185]\ttrain-F1_Weighted:0.84024\n",
      "[186]\ttrain-F1_Weighted:0.84184\n",
      "[187]\ttrain-F1_Weighted:0.84406\n",
      "[188]\ttrain-F1_Weighted:0.84532\n",
      "[189]\ttrain-F1_Weighted:0.84639\n",
      "[190]\ttrain-F1_Weighted:0.84821\n",
      "[191]\ttrain-F1_Weighted:0.85053\n",
      "[192]\ttrain-F1_Weighted:0.85131\n",
      "[193]\ttrain-F1_Weighted:0.85271\n",
      "[194]\ttrain-F1_Weighted:0.85347\n",
      "[195]\ttrain-F1_Weighted:0.85407\n",
      "[196]\ttrain-F1_Weighted:0.85522\n",
      "[197]\ttrain-F1_Weighted:0.85597\n",
      "[198]\ttrain-F1_Weighted:0.85749\n",
      "[199]\ttrain-F1_Weighted:0.85889\n",
      "[200]\ttrain-F1_Weighted:0.86016\n",
      "[201]\ttrain-F1_Weighted:0.86131\n",
      "[202]\ttrain-F1_Weighted:0.86180\n",
      "[203]\ttrain-F1_Weighted:0.86248\n",
      "[204]\ttrain-F1_Weighted:0.86367\n",
      "[205]\ttrain-F1_Weighted:0.86389\n",
      "[206]\ttrain-F1_Weighted:0.86477\n",
      "[207]\ttrain-F1_Weighted:0.86602\n",
      "[208]\ttrain-F1_Weighted:0.86734\n",
      "[209]\ttrain-F1_Weighted:0.86767\n",
      "[210]\ttrain-F1_Weighted:0.86845\n",
      "[211]\ttrain-F1_Weighted:0.86950\n",
      "[212]\ttrain-F1_Weighted:0.86993\n",
      "[213]\ttrain-F1_Weighted:0.87080\n",
      "[214]\ttrain-F1_Weighted:0.87220\n",
      "[215]\ttrain-F1_Weighted:0.87366\n",
      "[216]\ttrain-F1_Weighted:0.87420\n",
      "[217]\ttrain-F1_Weighted:0.87601\n",
      "[218]\ttrain-F1_Weighted:0.87702\n",
      "[219]\ttrain-F1_Weighted:0.87847\n",
      "[220]\ttrain-F1_Weighted:0.87951\n",
      "[221]\ttrain-F1_Weighted:0.88053\n",
      "[222]\ttrain-F1_Weighted:0.88168\n",
      "[223]\ttrain-F1_Weighted:0.88213\n",
      "[224]\ttrain-F1_Weighted:0.88362\n",
      "[225]\ttrain-F1_Weighted:0.88436\n",
      "[226]\ttrain-F1_Weighted:0.88558\n",
      "[227]\ttrain-F1_Weighted:0.88621\n",
      "[228]\ttrain-F1_Weighted:0.88685\n",
      "[229]\ttrain-F1_Weighted:0.88741\n",
      "[230]\ttrain-F1_Weighted:0.88783\n",
      "[231]\ttrain-F1_Weighted:0.88877\n",
      "[232]\ttrain-F1_Weighted:0.88939\n",
      "[233]\ttrain-F1_Weighted:0.89033\n",
      "[234]\ttrain-F1_Weighted:0.89144\n",
      "[235]\ttrain-F1_Weighted:0.89258\n",
      "[236]\ttrain-F1_Weighted:0.89349\n",
      "[237]\ttrain-F1_Weighted:0.89403\n",
      "[238]\ttrain-F1_Weighted:0.89550\n",
      "[239]\ttrain-F1_Weighted:0.89753\n",
      "[240]\ttrain-F1_Weighted:0.89820\n",
      "[241]\ttrain-F1_Weighted:0.89842\n",
      "[242]\ttrain-F1_Weighted:0.89938\n",
      "[243]\ttrain-F1_Weighted:0.89987\n",
      "[244]\ttrain-F1_Weighted:0.90101\n",
      "[245]\ttrain-F1_Weighted:0.90207\n",
      "[246]\ttrain-F1_Weighted:0.90282\n",
      "[247]\ttrain-F1_Weighted:0.90316\n",
      "[248]\ttrain-F1_Weighted:0.90350\n",
      "[249]\ttrain-F1_Weighted:0.90468\n",
      "[250]\ttrain-F1_Weighted:0.90577\n",
      "[251]\ttrain-F1_Weighted:0.90597\n",
      "[252]\ttrain-F1_Weighted:0.90604\n",
      "[253]\ttrain-F1_Weighted:0.90726\n",
      "[254]\ttrain-F1_Weighted:0.90794\n",
      "[255]\ttrain-F1_Weighted:0.90890\n",
      "[256]\ttrain-F1_Weighted:0.90912\n",
      "[257]\ttrain-F1_Weighted:0.90958\n",
      "[258]\ttrain-F1_Weighted:0.90998\n",
      "[259]\ttrain-F1_Weighted:0.91078\n",
      "[260]\ttrain-F1_Weighted:0.91170\n",
      "[261]\ttrain-F1_Weighted:0.91221\n",
      "[262]\ttrain-F1_Weighted:0.91322\n",
      "[263]\ttrain-F1_Weighted:0.91389\n",
      "[264]\ttrain-F1_Weighted:0.91451\n",
      "[265]\ttrain-F1_Weighted:0.91543\n",
      "[266]\ttrain-F1_Weighted:0.91589\n",
      "[267]\ttrain-F1_Weighted:0.91608\n",
      "[268]\ttrain-F1_Weighted:0.91662\n",
      "[269]\ttrain-F1_Weighted:0.91754\n",
      "[270]\ttrain-F1_Weighted:0.91763\n",
      "[271]\ttrain-F1_Weighted:0.91772\n",
      "[272]\ttrain-F1_Weighted:0.91824\n",
      "[273]\ttrain-F1_Weighted:0.91889\n",
      "[274]\ttrain-F1_Weighted:0.91896\n",
      "[275]\ttrain-F1_Weighted:0.91911\n",
      "[276]\ttrain-F1_Weighted:0.91943\n",
      "[277]\ttrain-F1_Weighted:0.92049\n",
      "[278]\ttrain-F1_Weighted:0.92135\n",
      "[279]\ttrain-F1_Weighted:0.92181\n",
      "[280]\ttrain-F1_Weighted:0.92272\n",
      "[281]\ttrain-F1_Weighted:0.92264\n",
      "[282]\ttrain-F1_Weighted:0.92324\n",
      "[283]\ttrain-F1_Weighted:0.92352\n",
      "[284]\ttrain-F1_Weighted:0.92425\n",
      "[285]\ttrain-F1_Weighted:0.92474\n",
      "[286]\ttrain-F1_Weighted:0.92488\n",
      "[287]\ttrain-F1_Weighted:0.92544\n",
      "[288]\ttrain-F1_Weighted:0.92603\n",
      "[289]\ttrain-F1_Weighted:0.92659\n",
      "[290]\ttrain-F1_Weighted:0.92711\n",
      "[291]\ttrain-F1_Weighted:0.92776\n",
      "[292]\ttrain-F1_Weighted:0.92847\n",
      "[293]\ttrain-F1_Weighted:0.92894\n",
      "[294]\ttrain-F1_Weighted:0.92898\n",
      "[295]\ttrain-F1_Weighted:0.92936\n",
      "[296]\ttrain-F1_Weighted:0.92956\n",
      "[297]\ttrain-F1_Weighted:0.93040\n",
      "[298]\ttrain-F1_Weighted:0.93136\n",
      "[299]\ttrain-F1_Weighted:0.93156\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    '''Softmax function with x as input vector.'''\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def softprob_obj(predt: np.ndarray, data: xgb.DMatrix):\n",
    "    '''Loss function.  Computing the gradient and approximated hessian (diagonal).\n",
    "    Reimplements the `multi:softprob` inside XGBoost.\n",
    "\n",
    "    '''\n",
    "    labels = data.get_label()\n",
    "    kRows = predt.shape[0]\n",
    "    kClasses = 5\n",
    "\n",
    "    if data.get_weight().size == 0:\n",
    "        # Use 1 as weight if we don't have custom weight.\n",
    "        weights = np.ones((kRows, 1), dtype=float)\n",
    "    else:\n",
    "        weights = data.get_weight()\n",
    "\n",
    "    # The prediction is of shape (rows, classes), each element in a row\n",
    "    # represents a raw prediction (leaf weight, hasn't gone through softmax\n",
    "    # yet).\n",
    "    assert predt.shape == (kRows, kClasses)\n",
    "\n",
    "    grad = np.zeros((kRows, kClasses), dtype=float)\n",
    "    hess = np.zeros((kRows, kClasses), dtype=float)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # compute the gradient and hessian, slow iterations in Python, only\n",
    "    # suitable for demo.  Also the one in native XGBoost core is more robust to\n",
    "    # numeric overflow as we don't do anything to mitigate the `exp` in\n",
    "    # `softmax` here.\n",
    "    for r in range(predt.shape[0]):\n",
    "        target = labels[r]\n",
    "        p = softmax(predt[r, :])\n",
    "        for c in range(predt.shape[1]):\n",
    "            assert target >= 0 or target <= kClasses\n",
    "            g = p[c] - 1.0 if c == target else p[c]\n",
    "            g = g * weights[r]\n",
    "            h = max((2.0 * p[c] * (1.0 - p[c]) * weights[r]).item(), eps)\n",
    "            grad[r, c] = g\n",
    "            hess[r, c] = h\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "def f1_weighted_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y_true = dtrain.get_label()\n",
    "    # Convert raw logits to predicted class\n",
    "    y_pred = np.argmax(predt, axis=1)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_Weighted', f1\n",
    "\n",
    "model = xgb.train(\n",
    "    {\n",
    "        'num_class':5,\n",
    "        'disable_default_eval_metric':True\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=300,\n",
    "    obj=softprob_obj,\n",
    "    custom_metric=f1_weighted_eval,\n",
    "    evals=[(m, 'train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- classification report ----------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.02      0.04       443\n",
      "           1       0.90      1.00      0.95      7891\n",
      "           2       0.40      0.03      0.06       426\n",
      "\n",
      "    accuracy                           0.90      8760\n",
      "   macro avg       0.55      0.35      0.35      8760\n",
      "weighted avg       0.85      0.90      0.86      8760\n",
      "\n",
      "\n",
      "-------------------- confusion matrix -------------------\n",
      "\n",
      "[[   9  430    4]\n",
      " [  13 7861   17]\n",
      " [   3  409   14]]\n",
      "\n",
      "\n",
      " trainging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      2799\n",
      "           1       0.99      1.00      1.00     29306\n",
      "           2       1.00      0.95      0.97      2932\n",
      "\n",
      "    accuracy                           0.99     35037\n",
      "   macro avg       1.00      0.97      0.98     35037\n",
      "weighted avg       0.99      0.99      0.99     35037\n",
      "\n",
      "[[ 2677   121     1]\n",
      " [    0 29306     0]\n",
      " [    0   147  2785]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(dtest)\n",
    "\n",
    "print('\\n----------------- classification report ----------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('\\n-------------------- confusion matrix -------------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('\\n\\n trainging')\n",
    "print(classification_report(y_train, model.predict(dtrain)))\n",
    "print(confusion_matrix(y_train, model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "XGBoost Model 2\n",
    "***\n",
    "\n",
    "Tuned using `Optuna`. See https://www.kaggle.com/code/para24/xgboost-stepwise-tuning-using-optuna/notebook#7.-Stepwise-Hyperparameter-Tuning\n",
    "\n",
    "We separate the 6 most effective and commonly altered XGBoost hyperparameters into 3 groups as follows:\n",
    "\n",
    "Group 1: `max_depth`, `min_child_weight`\n",
    "\n",
    "Group 2: `subsample`, `colsample_bytree`\n",
    "\n",
    "Group 3: `learning_rate`, `num_boost_round`\n",
    "\n",
    "We then tune each group sequentially, finding the optimal value for each group using previous findings (with `learning_rate` and `num_boost_round` set to some default values to begin with before being the final group to be optimised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['date', 'next_jump'], axis=1).copy()\n",
    "y = df['next_jump'].copy()\n",
    "y = y.map(jump_lookup)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study, logging\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "def objective(trial, X, y, group, score, params=dict()):\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "    # initial learning params\n",
    "    params['num_boost_round'] = 200\n",
    "    params['learning_rate'] = 0.01\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "\n",
    "    if group == '1':\n",
    "        params['max_depth'] = trial.suggest_int('max_depth', 2, 30)\n",
    "        params['min_child_weight'] = trial.suggest_loguniform('min_child_weight', 1e-10, 1e10)\n",
    "    \n",
    "    if group == '2':\n",
    "        params['subsample'] = trial.suggest_uniform('subsample', 0, 1)\n",
    "        params['colsample_bytree'] = trial.suggest_uniform('colsample_bytree', 0, 1)\n",
    "\n",
    "    if group == '3':\n",
    "        num_boost_round = trial.suggest_int('num_boost_round', 100, 400)\n",
    "        learning_rate = trial.suggest_uniform('learning_rate', 0.005, 0.1)\n",
    "        params['num_boost_round'] = num_boost_round\n",
    "        params['learning_rate'] = learning_rate\n",
    "\n",
    "    pruning_callback = XGBoostPruningCallback(trial, 'test-' + score.__name__)\n",
    "\n",
    "    cv_scores = xgb.cv(params, dtrain, nfold=5,\n",
    "                       stratified=True,\n",
    "                       feval=score,\n",
    "                       num_boost_round=params['num_boost_round'],\n",
    "                       early_stopping_rounds=10,\n",
    "                       callbacks=[pruning_callback])\n",
    "    \n",
    "    return cv_scores['test-' + score.__name__ + '-mean'].values[-1]\n",
    "\n",
    "def execute_optimization(study_name, group, score, trials, params=dict(), direction='maximize'):\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "    ## use pruner to skip trials that aren't doing so well\n",
    "    pruner = MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "    study = create_study(\n",
    "        direction=direction,\n",
    "        study_name=study_name,\n",
    "        storage='sqlite:///optuna.db',\n",
    "        load_if_exists=True,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, group, score, params),\n",
    "        n_trials=trials,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    print('STUDY NAME: ', study_name)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('EVALUATION METRIC: ', score.__name__)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('BEST CV SCORE: ', study.best_value)\n",
    "    print('-------------------------------------------------------')\n",
    "    print(f'OPTIMAL GROUP - {group} PARAMS: ', study.best_params)\n",
    "    print('-------------------------------------------------------')\n",
    "    print('BEST TRIAL', study.best_trial)\n",
    "    print('-------------------------------------------------------')\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "def f1_weighted_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = np.argmax(predt, axis=1)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_score', f1\n",
    "\n",
    "f1_weighted_eval.__name__ = f1_score.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Optimizing Group 1 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.4225478\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 1 PARAMS:  {'max_depth': 13, 'min_child_weight': 0.0003908866471641741}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=258, state=TrialState.COMPLETE, values=[0.4225478], datetime_start=datetime.datetime(2025, 1, 9, 23, 11, 16, 63561), datetime_complete=datetime.datetime(2025, 1, 9, 23, 11, 25, 813932), params={'max_depth': 13, 'min_child_weight': 0.0003908866471641741}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4230824, 1: 0.4225478, 2: 0.42300839999999995, 3: 0.42279900000000004, 4: 0.4237483999999999, 5: 0.4238528, 6: 0.4244124, 7: 0.4252078, 8: 0.42518899999999993, 9: 0.4248296, 10: 0.42467160000000004}, distributions={'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_child_weight': FloatDistribution(high=10000000000.0, log=True, low=1e-10, step=None)}, trial_id=259, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 1:  {'num_boost_round': 200, 'learning_rate': 0.01, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 13, 'min_child_weight': 0.0003908866471641741}\n",
      "\n",
      "\n",
      "\n",
      "====== Optimizing Group 2 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.4225478\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 2 PARAMS:  {'max_depth': 13, 'min_child_weight': 0.0003908866471641741}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=258, state=TrialState.COMPLETE, values=[0.4225478], datetime_start=datetime.datetime(2025, 1, 9, 23, 11, 16, 63561), datetime_complete=datetime.datetime(2025, 1, 9, 23, 11, 25, 813932), params={'max_depth': 13, 'min_child_weight': 0.0003908866471641741}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4230824, 1: 0.4225478, 2: 0.42300839999999995, 3: 0.42279900000000004, 4: 0.4237483999999999, 5: 0.4238528, 6: 0.4244124, 7: 0.4252078, 8: 0.42518899999999993, 9: 0.4248296, 10: 0.42467160000000004}, distributions={'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_child_weight': FloatDistribution(high=10000000000.0, log=True, low=1e-10, step=None)}, trial_id=259, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 2:  {'num_boost_round': 200, 'learning_rate': 0.01, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 13, 'min_child_weight': 0.0003908866471641741, 'subsample': 0.9825792288285299, 'colsample_bytree': 0.8523491029759179}\n",
      "\n",
      "\n",
      "\n",
      "====== Optimizing Group 3 ======\n",
      "STUDY NAME:  xgboost\n",
      "-------------------------------------------------------\n",
      "EVALUATION METRIC:  f1_score\n",
      "-------------------------------------------------------\n",
      "BEST CV SCORE:  0.4225478\n",
      "-------------------------------------------------------\n",
      "OPTIMAL GROUP - 3 PARAMS:  {'max_depth': 13, 'min_child_weight': 0.0003908866471641741}\n",
      "-------------------------------------------------------\n",
      "BEST TRIAL FrozenTrial(number=258, state=TrialState.COMPLETE, values=[0.4225478], datetime_start=datetime.datetime(2025, 1, 9, 23, 11, 16, 63561), datetime_complete=datetime.datetime(2025, 1, 9, 23, 11, 25, 813932), params={'max_depth': 13, 'min_child_weight': 0.0003908866471641741}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4230824, 1: 0.4225478, 2: 0.42300839999999995, 3: 0.42279900000000004, 4: 0.4237483999999999, 5: 0.4238528, 6: 0.4244124, 7: 0.4252078, 8: 0.42518899999999993, 9: 0.4248296, 10: 0.42467160000000004}, distributions={'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_child_weight': FloatDistribution(high=10000000000.0, log=True, low=1e-10, step=None)}, trial_id=259, value=None)\n",
      "-------------------------------------------------------\n",
      "Params after updating group 3:  {'num_boost_round': 291, 'learning_rate': 0.08804971815260643, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 13, 'min_child_weight': 0.0003908866471641741, 'subsample': 0.9825792288285299, 'colsample_bytree': 0.8523491029759179}\n",
      "\n",
      "\n",
      "\n",
      "====== Final Optimal Parameters ======\n",
      "{'num_boost_round': 291, 'learning_rate': 0.08804971815260643, 'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 13, 'min_child_weight': 0.0003908866471641741, 'subsample': 0.9825792288285299, 'colsample_bytree': 0.8523491029759179}\n"
     ]
    }
   ],
   "source": [
    "def stepwise_optimization(trials=10):\n",
    "    final_params = dict()\n",
    "    for g in ['1', '2', '3']:\n",
    "        print(f'====== Optimizing Group {g} ======')\n",
    "        update_params = execute_optimization(\n",
    "            'xgboost', g, f1_weighted_eval, trials, params=final_params, direction='maximize'\n",
    "        )\n",
    "        final_params.update(update_params)\n",
    "        print(f'Params after updating group {g}: ', final_params)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    print(f'====== Final Optimal Parameters ======')\n",
    "    print(final_params)\n",
    "\n",
    "    return final_params\n",
    "\n",
    "xgb2_params = stepwise_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0xff9e8d71bdf0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2_params['num_class'] = 3\n",
    "\n",
    "model = xgb.train(\n",
    "    params=xgb2_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=xgb2_params['num_boost_round']\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "\n",
      "----------------- classification report ----------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.01      0.02       443\n",
      "           1       0.90      1.00      0.95      7891\n",
      "           2       0.60      0.05      0.09       426\n",
      "\n",
      "    accuracy                           0.90      8760\n",
      "   macro avg       0.61      0.35      0.35      8760\n",
      "weighted avg       0.86      0.90      0.86      8760\n",
      "\n",
      "\n",
      "-------------------- confusion matrix -------------------\n",
      "\n",
      "[[   4  438    1]\n",
      " [   7 7871   13]\n",
      " [   1  404   21]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print('\\n----------------- classification report ----------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('\\n------------------- confusion matrix -------------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
